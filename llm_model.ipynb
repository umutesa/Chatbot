{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u82qiQ8JpgfK",
        "outputId": "44fc1af4-0e49-4dc3-c7c0-22b5b9163f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.56 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.56)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.24)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.39)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain_community) (2.11.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain_community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.56->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.23-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.23 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtSkb47brp29",
        "outputId": "7dee890a-d229-46f2-ee17-fdeac6b0c6f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOIaO_R3AdC7",
        "outputId": "95931b5b-7f38-499f-cbda-d014f02bca1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzNSaTerqfcO",
        "outputId": "dbf38d6c-ef45-4816-a353-0beb8ae52767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWCt6Rpjpq-y",
        "outputId": "a828cace-881f-4dc4-e336-15ce532c2901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.16-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.58 (from langchain_openai)\n",
            "  Downloading langchain_core-0.3.59-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.76.2)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain_openai) (0.3.39)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain_openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain_openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain_openai) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain_openai) (2.11.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.58->langchain_openai) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.58->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.58->langchain_openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.58->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.58->langchain_openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.58->langchain_openai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.4.0)\n",
            "Downloading langchain_openai-0.3.16-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.59-py3-none-any.whl (437 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.7/437.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain-core, langchain_openai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.56\n",
            "    Uninstalling langchain-core-0.3.56:\n",
            "      Successfully uninstalled langchain-core-0.3.56\n",
            "Successfully installed langchain-core-0.3.59 langchain_openai-0.3.16 tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jrqU3psqDHo",
        "outputId": "878bf97d-d813-4623-a290-8512854e84e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.5.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Downloading pypdf-5.5.0-py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSLYX7zipZvi",
        "outputId": "25f13d4c-73ee-4c91-929f-02e13b3958f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.59)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.39)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVfm-XD10n_l"
      },
      "outputs": [],
      "source": [
        "#--------------------LIABARIES-------------------------------------------\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import json\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any, Optional, Union\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.docstore.document import Document\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from typing import Tuple, List, Dict\n",
        "from langchain.retrievers import MultiQueryRetriever\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import kagglehub\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Tuple, Any"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------------------------Download Dataset--------------------------------------------\n",
        "\n",
        "path = kagglehub.dataset_download(\"crispen5gar/recipes3k\")\n",
        "json_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith(\".json\")]\n",
        "for file in json_files:\n",
        "    print(file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObmJpUD7k5aq",
        "outputId": "5330311d-be80-42a4-83bb-40d66add1a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/crispen5gar/recipes3k?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.53M/1.53M [00:00<00:00, 116MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "/root/.cache/kagglehub/datasets/crispen5gar/recipes3k/versions/1/baking.json\n",
            "/root/.cache/kagglehub/datasets/crispen5gar/recipes3k/versions/1/inspiration.json\n",
            "/root/.cache/kagglehub/datasets/crispen5gar/recipes3k/versions/1/budget.json\n",
            "/root/.cache/kagglehub/datasets/crispen5gar/recipes3k/versions/1/health.json\n",
            "/root/.cache/kagglehub/datasets/crispen5gar/recipes3k/versions/1/recipes.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-----------------------------------------------------------------Defining Model-------------------------------------------------\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "\n",
        "\"\"\"Class to store and manage human feedback for reinforcement learning.\"\"\"\n",
        "class RLHFFeedback:\n",
        "    def __init__(self, feedback_file: str = \"rlhf_feedback.json\"):\n",
        "        self.feedback_file = feedback_file\n",
        "        self.feedback_data = self._load_feedback()\n",
        "\n",
        "    def _load_feedback(self) -> Dict:\n",
        "        \"\"\"Load existing feedback data or create a new structure.\"\"\"\n",
        "        if os.path.exists(self.feedback_file):\n",
        "            try:\n",
        "                with open(self.feedback_file, 'r') as f:\n",
        "                    return json.load(f)\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Error loading feedback file. Creating new feedback database.\")\n",
        "\n",
        "        # Initial structure\n",
        "        return {\n",
        "            \"interactions\": [],\n",
        "            \"preferred_responses\": [],\n",
        "            \"rejected_responses\": [],\n",
        "            \"model_updates\": []\n",
        "        }\n",
        "\n",
        "    def save_feedback(self):\n",
        "        \"\"\"Save feedback data to file.\"\"\"\n",
        "        with open(self.feedback_file, 'w') as f:\n",
        "            json.dump(self.feedback_data, f, indent=2)\n",
        "        print(f\"Feedback saved to {self.feedback_file}\")\n",
        "\n",
        "    def add_interaction(self, query: str, response: str, source_types: List[str]):\n",
        "        \"\"\"Add a new interaction without feedback yet.\"\"\"\n",
        "        self.feedback_data[\"interactions\"].append({\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"query\": query,\n",
        "            \"response\": response,\n",
        "            \"source_types\": source_types,\n",
        "            \"feedback_requested\": True,\n",
        "            \"feedback_provided\": False\n",
        "        })\n",
        "        self.save_feedback()\n",
        "        return len(self.feedback_data[\"interactions\"]) - 1  # Return index for reference\n",
        "\n",
        "    def add_feedback(self, interaction_idx: int, rating: int, alternative_response: Optional[str] = None):\n",
        "        \"\"\"Add human feedback to an existing interaction.\"\"\"\n",
        "        if 0 <= interaction_idx < len(self.feedback_data[\"interactions\"]):\n",
        "            interaction = self.feedback_data[\"interactions\"][interaction_idx]\n",
        "            interaction[\"feedback_provided\"] = True\n",
        "            interaction[\"rating\"] = rating\n",
        "            interaction[\"feedback_timestamp\"] = datetime.now().isoformat()\n",
        "\n",
        "            if rating >= 4:  # Rating of 4 or 5 (on a 1-5 scale)\n",
        "                self.feedback_data[\"preferred_responses\"].append({\n",
        "                    \"query\": interaction[\"query\"],\n",
        "                    \"response\": interaction[\"response\"],\n",
        "                    \"rating\": rating,\n",
        "                    \"source_types\": interaction[\"source_types\"]\n",
        "                })\n",
        "            elif rating <= 2:  # Rating of 1 or 2\n",
        "                self.feedback_data[\"rejected_responses\"].append({\n",
        "                    \"query\": interaction[\"query\"],\n",
        "                    \"response\": interaction[\"response\"],\n",
        "                    \"rating\": rating,\n",
        "                    \"source_types\": interaction[\"source_types\"],\n",
        "                    \"alternative_response\": alternative_response\n",
        "                })\n",
        "\n",
        "            self.save_feedback()\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def get_training_pairs(self, min_samples: int = 5) -> Tuple[List[Dict], List[Dict], bool]:\n",
        "        \"\"\"Get training data pairs for RLHF fine-tuning.\"\"\"\n",
        "        preferred = self.feedback_data[\"preferred_responses\"]\n",
        "        rejected = self.feedback_data[\"rejected_responses\"]\n",
        "\n",
        "        # Only return data if we have enough samples\n",
        "        if len(preferred) >= min_samples and len(rejected) >= min_samples:\n",
        "            return preferred, rejected, True\n",
        "\n",
        "        return [], [], False\n",
        "\n",
        "    def record_model_update(self, update_details: Dict):\n",
        "        \"\"\"Record when the model was updated with RLHF.\"\"\"\n",
        "        update_record = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"details\": update_details,\n",
        "            \"samples_used\": {\n",
        "                \"preferred\": len(self.feedback_data[\"preferred_responses\"]),\n",
        "                \"rejected\": len(self.feedback_data[\"rejected_responses\"])\n",
        "            }\n",
        "        }\n",
        "        self.feedback_data[\"model_updates\"].append(update_record)\n",
        "        self.save_feedback()\n",
        "\n",
        "\n",
        "class HybridChatbot:\n",
        "    def __init__(self,\n",
        "                 paper_url: str = \"https://yourknow.com/uploads/books/5dd0f9604c895.pdf\",\n",
        "                 dataset_path: str = None,\n",
        "                 model_name: str = \"gpt2-medium\",\n",
        "                 embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                 vector_db_path: str = \"hybrid_vectors\",\n",
        "                 enable_rlhf: bool = True,\n",
        "                 feedback_file: str = \"rlhf_feedback.json\",\n",
        "                 rlhf_update_threshold: int = 20):\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.vector_db_path = vector_db_path\n",
        "        self.paper_url = paper_url\n",
        "        self.enable_rlhf = enable_rlhf\n",
        "        self.rlhf_update_threshold = rlhf_update_threshold\n",
        "\n",
        "        print(\"Initializing CPU-optimized Chatbot...\")\n",
        "\n",
        "        # Initialize RLHF feedback system\n",
        "        if self.enable_rlhf:\n",
        "            self.feedback_system = RLHFFeedback(feedback_file)\n",
        "\n",
        "        # 1. Load and prepare dataset\n",
        "        if dataset_path:\n",
        "            self.dataset_path = dataset_path\n",
        "        else:\n",
        "            # Download the dataset from Kaggle\n",
        "            self.dataset_path = self._download_dataset()\n",
        "\n",
        "        self._intents = self._load_dataset()\n",
        "\n",
        "        # 2. Load PDF and prepare for document retrieval\n",
        "        self.pdf_docs = self._load_and_split_pdf()\n",
        "\n",
        "        # 3. Initialize embedding model - with CPU optimizations\n",
        "        self.embedding_model = HuggingFaceEmbeddings(\n",
        "            model_name=embedding_model,\n",
        "            model_kwargs={\"device\": \"cpu\"},\n",
        "            encode_kwargs={\"batch_size\": 8}  # Smaller batch size for CPU\n",
        "        )\n",
        "\n",
        "        # 4. Create/load vector database for retrieval\n",
        "        self.vector_db = self._setup_vector_db()\n",
        "\n",
        "        # 5. Initialize language model with CPU optimizations\n",
        "        print(f\"Loading language model: {model_name}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        # Force CPU and use half precision to save memory\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"cpu\",\n",
        "            torch_dtype=torch.float32  # Using float32 for CPU compatibility\n",
        "        )\n",
        "\n",
        "        # 6. Set up the pipeline with optimized settings for CPU\n",
        "        self.llm_pipeline = self._setup_llm_pipeline()\n",
        "\n",
        "        # 7. Create QA chain\n",
        "        self.qa_chain = self._setup_qa_chain()\n",
        "\n",
        "        print(\"Chatbot initialization complete!\")\n",
        "\n",
        "    def _download_dataset(self):\n",
        "        \"\"\"Download the dataset from Kaggle.\"\"\"\n",
        "        try:\n",
        "            path = kagglehub.dataset_download(\"crispen5gar/recipes3k\")\n",
        "            data_file = os.path.join(path, \"recipes.json\")\n",
        "            return data_file\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading dataset: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _load_dataset(self):\n",
        "        \"\"\"Load the dataset knowledge base.\"\"\"\n",
        "        try:\n",
        "            print(f\"Loading dataset from: {self.dataset_path}\")\n",
        "            with open(self.dataset_path, 'r') as file:\n",
        "                intents = json.load(file)\n",
        "            return intents\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading Dataset knowledge base: {e}\")\n",
        "            # Create empty intents if loading fails\n",
        "            return {\"intents\": []}\n",
        "\n",
        "    def _load_and_split_pdf(self):\n",
        "        \"\"\"Load and split the research paper PDF.\"\"\"\n",
        "        try:\n",
        "            print(f\"Loading PDF from: {self.paper_url}\")\n",
        "            loader = PyPDFLoader(self.paper_url)\n",
        "            pages = loader.load()\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=2000,\n",
        "                chunk_overlap=700\n",
        "            )\n",
        "            return text_splitter.split_documents(pages)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading PDF: {e}\")\n",
        "            return []  # Return empty list if loading fails\n",
        "\n",
        "    def _convert_dataset_to_documents(self):\n",
        "        \"\"\"Convert intents to Document objects for vector storage.\"\"\"\n",
        "        documents = []\n",
        "        for intent in self._intents:  # Remove the .get() since self._intents is already a list\n",
        "          for pattern in intent.get(\"patterns\", []):\n",
        "            documents.append(Document(page_content=pattern, metadata={\"source\": \"dataset_kb\", **intent}))\n",
        "        return documents\n",
        "\n",
        "    def _setup_vector_db(self):\n",
        "        \"\"\"Set up the vector database for hybrid retrieval.\"\"\"\n",
        "        # Check if vector database already exists\n",
        "        if os.path.exists(self.vector_db_path):\n",
        "            print(f\"Loading existing vector database from: {self.vector_db_path}\")\n",
        "            return FAISS.load_local(self.vector_db_path, self.embedding_model, allow_dangerous_deserialization=True)\n",
        "\n",
        "        print(\"Creating new vector database...\")\n",
        "        # Combine PDF documents and dataset documents\n",
        "        dataset_docs = self._convert_dataset_to_documents()\n",
        "        all_docs = self.pdf_docs + dataset_docs\n",
        "\n",
        "        # Process in smaller batches to optimize for CPU\n",
        "        batch_size = 32\n",
        "        all_batches = [all_docs[i:i + batch_size] for i in range(0, len(all_docs), batch_size)]\n",
        "\n",
        "        # Create vector database with the first batch\n",
        "        if len(all_batches) > 0 and len(all_batches[0]) > 0:\n",
        "            vector_db = FAISS.from_documents(all_batches[0], self.embedding_model)\n",
        "\n",
        "            # Add remaining batches\n",
        "            for i, batch in enumerate(all_batches[1:], 1):\n",
        "                if batch:  # Check if batch is not empty\n",
        "                    print(f\"Processing batch {i+1}/{len(all_batches)}...\")\n",
        "                    vector_db.add_documents(batch)\n",
        "        else:\n",
        "            # Fallback if no documents were processed\n",
        "            print(\"Warning: No documents to process for vector database\")\n",
        "            vector_db = FAISS.from_texts([\"Empty database placeholder\"], self.embedding_model)\n",
        "\n",
        "        # Save for future use\n",
        "        print(f\"Saving vector database to: {self.vector_db_path}\")\n",
        "        vector_db.save_local(self.vector_db_path)\n",
        "\n",
        "        return vector_db\n",
        "\n",
        "    def _setup_llm_pipeline(self):\n",
        "        \"\"\"Set up the language model pipeline optimized for CPU.\"\"\"\n",
        "        print(\"Setting up language model pipeline...\")\n",
        "        # CPU-optimized parameters: Lower max_length, increase pad_token_id consistency\n",
        "        llm_pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            max_length=6000,  # Reduced for CPU efficiency\n",
        "            min_length=600,\n",
        "            temperature=0.30,\n",
        "            top_p=0.95,\n",
        "            top_k=50,\n",
        "            repetition_penalty=1.2,\n",
        "            do_sample=True,\n",
        "            pad_token_id=self.tokenizer.eos_token_id, # Force CPU\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return HuggingFacePipeline(pipeline=llm_pipeline)\n",
        "\n",
        "    def _setup_qa_chain(self):\n",
        "        \"\"\"Set up the QA chain with prompt template.\"\"\"\n",
        "        # Create prompt template\n",
        "        prompt_template = \"\"\"\n",
        "        I am a Chatbot assistant that can answer questions about South African Food recipes and some general recipes!\n",
        "\n",
        "        {context}\n",
        "\n",
        "        QUESTION:\n",
        "        {question}\n",
        "\n",
        "        ANSWER:\n",
        "        \"\"\"\n",
        "\n",
        "        prompt = PromptTemplate(\n",
        "            template=prompt_template,\n",
        "            input_variables=[\"context\", \"question\"]\n",
        "        )\n",
        "\n",
        "        return RetrievalQA.from_chain_type(\n",
        "            llm=self.llm_pipeline,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=self.vector_db.as_retriever(search_kwargs={\"k\": 3}),\n",
        "            return_source_documents=True,\n",
        "            chain_type_kwargs={\"prompt\": prompt}\n",
        "        )\n",
        "\n",
        "\n",
        "    def format_answer(self, answer: str) -> str:\n",
        "      \"\"\"Clean and format the answer.\"\"\"\n",
        "      unwanted_phrases = [\n",
        "        \"I am a Chatbot assistant that can\",\n",
        "        \"QUESTION:\",\n",
        "        \"ANSWER:\",\n",
        "      ]\n",
        "      for phrase in unwanted_phrases:\n",
        "        answer = answer.replace(phrase, \"\")\n",
        "\n",
        "    # Trim leading/trailing whitespace\n",
        "      answer = answer\n",
        "\n",
        "    # Ensure a complete sentence\n",
        "      if not answer.endswith(\".\"):\n",
        "        answer += \".\"\n",
        "\n",
        "      return answer\n",
        "\n",
        "    def get_answer111(self, query: str) -> Dict[str, Any]:\n",
        "      \"\"\"Get answer for the given query.\"\"\"\n",
        "      try:\n",
        "            # Set up retrievers for both dataset and paper\n",
        "        dataset_retriever = self.vector_db.as_retriever(search_kwargs={\"k\": 4, \"filter\": {\"source\": \"dataset_kb\"}})\n",
        "        paper_retriever = self.vector_db.as_retriever(search_kwargs={\"k\": 4, \"filter\": {\"source\": \"paper\"}})\n",
        "\n",
        "        self.qa_chain.retriever = paper_retriever\n",
        "        if not self.qa_chain.retriever:\n",
        "          self.qa_chain.retriever = dataset_retriever\n",
        "\n",
        "        # Generate answer\n",
        "        result = self.qa_chain({\"query\": query})\n",
        "        answer = result.get('result', '')\n",
        "\n",
        "        # Format the answer\n",
        "        answer = answer\n",
        "\n",
        "        # Identify source type\n",
        "        source_types = []\n",
        "        for doc in result.get('source_documents', []):\n",
        "            source = doc.metadata.get('source', '')\n",
        "            if source == \"dataset_kb\":\n",
        "                source_types.append(\"dataset\")\n",
        "            else:\n",
        "                source_types.append(\"paper\")\n",
        "\n",
        "        # Deduplicate source types\n",
        "        source_types = list(set(source_types))\n",
        "\n",
        "        # Store interaction for feedback\n",
        "        interaction_idx = -1\n",
        "        if self.enable_rlhf:\n",
        "            interaction_idx = self.feedback_system.add_interaction(query, answer, source_types)\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"source_types\": source_types,\n",
        "            \"interaction_idx\": interaction_idx\n",
        "        }\n",
        "      except Exception as e:\n",
        "        print(f\"Error processing query: {e}\")\n",
        "        return {\n",
        "            \"answer\": \"I'm sorry, I encountered an error while processing your question.\",\n",
        "            \"source_types\": [],\n",
        "            \"interaction_idx\": -1\n",
        "        }\n",
        "\n",
        "    def provide_feedback(self, interaction_idx: int, rating: int, alternative_response: Optional[str] = None) -> bool:\n",
        "        \"\"\"Add human feedback for a specific interaction.\"\"\"\n",
        "        if not self.enable_rlhf:\n",
        "            return False\n",
        "\n",
        "        success = self.feedback_system.add_feedback(interaction_idx, rating, alternative_response)\n",
        "\n",
        "        # Check if we should update the model based on feedback\n",
        "        if success:\n",
        "            preferred, rejected, should_update = self.feedback_system.get_training_pairs(\n",
        "                min_samples=self.rlhf_update_threshold\n",
        "            )\n",
        "\n",
        "            if should_update:\n",
        "                print(f\"Sufficient feedback collected ({len(preferred)} preferred, {len(rejected)} rejected). Updating model...\")\n",
        "                self._update_model_with_rlhf(preferred, rejected)\n",
        "            else:\n",
        "                remaining = self.rlhf_update_threshold - min(len(preferred), len(rejected))\n",
        "                print(f\"Need {remaining} more feedback samples before model update\")\n",
        "\n",
        "        return success\n",
        "\n",
        "    def _prepare_rlhf_training_data(self, preferred: List[Dict], rejected: List[Dict]):\n",
        "        \"\"\"Prepare data for RLHF training.\"\"\"\n",
        "        # Create paired examples for preference learning\n",
        "        training_pairs = []\n",
        "\n",
        "        # This simplified approach pairs random preferred and rejected responses\n",
        "        # Limit the number of pairs for CPU efficiency\n",
        "        max_pairs = min(len(preferred), len(rejected), 50)  # Cap at 50 pairs for CPU efficiency\n",
        "\n",
        "        for _ in range(max_pairs):\n",
        "            pref = random.choice(preferred)\n",
        "            rej = random.choice(rejected)\n",
        "\n",
        "            training_pairs.append({\n",
        "                \"query\": pref[\"query\"],\n",
        "                \"chosen\": pref[\"response\"],\n",
        "                \"rejected\": rej[\"response\"]\n",
        "            })\n",
        "\n",
        "        return training_pairs\n",
        "\n",
        "    def _update_model_with_rlhf(self, preferred: List[Dict], rejected: List[Dict]):\n",
        "        \"\"\"Update the language model using RLHF techniques - optimized for CPU.\"\"\"\n",
        "        try:\n",
        "            print(\"Starting RLHF model update with CPU optimizations...\")\n",
        "\n",
        "            # 1. Prepare data\n",
        "            training_pairs = self._prepare_rlhf_training_data(preferred, rejected)\n",
        "\n",
        "            # 2. Create a custom dataset for preference learning\n",
        "            class PreferenceDataset(torch.utils.data.Dataset):\n",
        "                def __init__(self, pairs, tokenizer, max_length=256):  # Reduced max_length for CPU\n",
        "                    self.pairs = pairs\n",
        "                    self.tokenizer = tokenizer\n",
        "                    self.max_length = max_length\n",
        "\n",
        "                def __len__(self):\n",
        "                    return len(self.pairs)\n",
        "\n",
        "                def __getitem__(self, idx):\n",
        "                    pair = self.pairs[idx]\n",
        "\n",
        "                    # Format inputs\n",
        "                    chosen_text = f\"Query: {pair['query']}\\nResponse: {pair['chosen']}\"\n",
        "                    rejected_text = f\"Query: {pair['query']}\\nResponse: {pair['rejected']}\"\n",
        "\n",
        "                    # Tokenize with reduced padding for CPU efficiency\n",
        "                    chosen_tokens = self.tokenizer(\n",
        "                        chosen_text,\n",
        "                        truncation=True,\n",
        "                        max_length=self.max_length,\n",
        "                        padding=\"max_length\",\n",
        "                        return_tensors=\"pt\"\n",
        "                    )\n",
        "\n",
        "                    rejected_tokens = self.tokenizer(\n",
        "                        rejected_text,\n",
        "                        truncation=True,\n",
        "                        max_length=self.max_length,\n",
        "                        padding=\"max_length\",\n",
        "                        return_tensors=\"pt\"\n",
        "                    )\n",
        "\n",
        "                    # Create preference labels - 1 for chosen, 0 for rejected\n",
        "                    return {\n",
        "                        \"chosen_input_ids\": chosen_tokens.input_ids.squeeze(),\n",
        "                        \"chosen_attention_mask\": chosen_tokens.attention_mask.squeeze(),\n",
        "                        \"rejected_input_ids\": rejected_tokens.input_ids.squeeze(),\n",
        "                        \"rejected_attention_mask\": rejected_tokens.attention_mask.squeeze(),\n",
        "                    }\n",
        "\n",
        "            # 3. Create dataset\n",
        "            dataset = PreferenceDataset(training_pairs, self.tokenizer)\n",
        "            print(f\"Created preference dataset with {len(dataset)} examples\")\n",
        "\n",
        "            # Create temporary model directory\n",
        "            output_dir = \"rlhf_updated_model\"\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "            # 4. Training arguments optimized for CPU\n",
        "            training_args = TrainingArguments(\n",
        "                output_dir=output_dir,\n",
        "                num_train_epochs=2,  # Reduced for CPU\n",
        "                per_device_train_batch_size=1,  # Smaller batch size for CPU\n",
        "                gradient_accumulation_steps=4,  # Accumulate gradients to compensate for small batch size\n",
        "                save_steps=50,\n",
        "                save_total_limit=1,  # Save disk space\n",
        "                logging_steps=10,\n",
        "                learning_rate=2e-5,  # Lower learning rate for stability\n",
        "                weight_decay=0.01,  # Regularization\n",
        "                no_cuda=True,  # Force CPU\n",
        "                dataloader_num_workers=1,  # Limit workers for CPU\n",
        "                evaluation_strategy=\"no\",  # No eval to save time\n",
        "                fp16=False,  # No mixed precision on CPU\n",
        "            )\n",
        "\n",
        "            # 5. Define training logic\n",
        "            class PreferenceTrainer(Trainer):\n",
        "                def compute_loss(self, model, inputs, return_outputs=False):\n",
        "                    # Process in smaller chunks to avoid OOM on CPU\n",
        "                    # Get logits for chosen and rejected responses\n",
        "                    chosen_outputs = model(\n",
        "                        input_ids=inputs[\"chosen_input_ids\"],\n",
        "                        attention_mask=inputs[\"chosen_attention_mask\"]\n",
        "                    )\n",
        "                    rejected_outputs = model(\n",
        "                        input_ids=inputs[\"rejected_input_ids\"],\n",
        "                        attention_mask=inputs[\"rejected_attention_mask\"]\n",
        "                    )\n",
        "\n",
        "                    # Extract logits\n",
        "                    chosen_logits = chosen_outputs.logits\n",
        "                    rejected_logits = rejected_outputs.logits\n",
        "\n",
        "                    # Simple preference loss: make chosen more likely than rejected\n",
        "                    # This is a simplified version of RLHF loss\n",
        "                    chosen_log_probs = self._get_sequence_log_probs(\n",
        "                        chosen_logits, inputs[\"chosen_input_ids\"], inputs[\"chosen_attention_mask\"]\n",
        "                    )\n",
        "                    rejected_log_probs = self._get_sequence_log_probs(\n",
        "                        rejected_logits, inputs[\"rejected_input_ids\"], inputs[\"rejected_attention_mask\"]\n",
        "                    )\n",
        "\n",
        "                    # Preference loss: maximize difference between chosen and rejected\n",
        "                    # Using stable softplus for numerical stability on CPU\n",
        "                    logits_diff = chosen_log_probs - rejected_log_probs\n",
        "                    loss = -torch.mean(torch.log(torch.sigmoid(logits_diff) + 1e-5))\n",
        "\n",
        "                    if return_outputs:\n",
        "                        return loss, (chosen_outputs, rejected_outputs)\n",
        "                    return loss\n",
        "\n",
        "                def _get_sequence_log_probs(self, logits, input_ids, attention_mask):\n",
        "                    # Calculate log probs for the sequence with memory optimizations\n",
        "                    shift_logits = logits[..., :-1, :].contiguous()\n",
        "                    shift_labels = input_ids[..., 1:].contiguous()\n",
        "\n",
        "                    # Get log probs using log_softmax with memory-efficient chunks\n",
        "                    log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)\n",
        "\n",
        "                    # Gather the log probs corresponding to the labels\n",
        "                    gathered_log_probs = log_probs.gather(-1, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "                    # Mask out padding\n",
        "                    shift_mask = attention_mask[..., 1:].contiguous()\n",
        "                    masked_log_probs = gathered_log_probs * shift_mask\n",
        "\n",
        "                    # Sum log probs and normalize by sequence length\n",
        "                    seq_lengths = torch.sum(shift_mask, dim=-1)\n",
        "                    seq_log_probs = torch.sum(masked_log_probs, dim=-1) / (seq_lengths + 1e-6)\n",
        "\n",
        "                    return seq_log_probs\n",
        "\n",
        "            # 6. Initialize trainer\n",
        "            trainer = PreferenceTrainer(\n",
        "                model=self.model,\n",
        "                args=training_args,\n",
        "                train_dataset=dataset,\n",
        "                tokenizer=self.tokenizer\n",
        "            )\n",
        "\n",
        "            # 7. Train the model with progress reporting\n",
        "            print(\"Starting training...\")\n",
        "            trainer.train()\n",
        "            print(\"Training complete!\")\n",
        "\n",
        "            # 8. Save and update the model\n",
        "            trainer.save_model(output_dir)\n",
        "            print(f\"RLHF-updated model saved to {output_dir}\")\n",
        "\n",
        "            # 9. Update the pipeline with new model - reload with CPU optimizations\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                output_dir,\n",
        "                device_map=\"cpu\",\n",
        "                torch_dtype=torch.float32\n",
        "            )\n",
        "            self.llm_pipeline = self._setup_llm_pipeline()\n",
        "            self.qa_chain = self._setup_qa_chain()\n",
        "\n",
        "            # 10. Record the update\n",
        "            self.feedback_system.record_model_update({\n",
        "                \"update_time\": datetime.now().isoformat(),\n",
        "                \"training_pairs\": len(training_pairs),\n",
        "                \"training_epochs\": training_args.num_train_epochs\n",
        "            })\n",
        "\n",
        "            print(\"Model successfully updated with RLHF!\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during RLHF update: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return False\n",
        "\n",
        "    def chat(self):\n",
        "        \"\"\"Interactive chat mode with RLHF feedback collection.\"\"\"\n",
        "        print(\"This chatbot can answer questions about South African Food recipes\")\n",
        "        print(\"Type 'exit' to quit\")\n",
        "        if self.enable_rlhf:\n",
        "            print(\"Type 'feedback' after a response to provide feedback\")\n",
        "        print(\"===================================\\n\")\n",
        "\n",
        "        while True:\n",
        "            user_input = input(\"You: \")\n",
        "            if user_input.lower() in ['exit', 'quit', 'bye']:\n",
        "                print(\"Chatbot: Goodbye!\")\n",
        "                break\n",
        "\n",
        "            if user_input.lower() == 'feedback' and self.enable_rlhf and hasattr(self, 'last_interaction_idx'):\n",
        "                # Get feedback\n",
        "                try:\n",
        "                    rating = int(input(\"Please rate the last response (1-5, 5 being best): \"))\n",
        "                    if 1 <= rating <= 5:\n",
        "                        alt_response = None\n",
        "                        if rating <= 2:  # For negative ratings, ask for alternative response\n",
        "                            alt_response = input(\"Please suggest a better response (optional): \")\n",
        "                            if not alt_response.strip():\n",
        "                                alt_response = None\n",
        "\n",
        "                        success = self.provide_feedback(self.last_interaction_idx, rating, alt_response)\n",
        "                        if success:\n",
        "                            print(\"Thank you for your feedback!\")\n",
        "                        else:\n",
        "                            print(\"Failed to record feedback.\")\n",
        "                    else:\n",
        "                        print(\"Invalid rating. Please use a scale of 1-5.\")\n",
        "                except ValueError:\n",
        "                    print(\"Invalid input. Please enter a number between 1 and 5.\")\n",
        "                continue\n",
        "\n",
        "            response = self.get_answer(user_input)\n",
        "            print(f\"\\nChatbot: {response['answer']}\\n\")\n",
        "\n",
        "            # Store interaction index for feedback\n",
        "            if self.enable_rlhf:\n",
        "                self.last_interaction_idx = response['interaction_idx']\n",
        "                print(\"(Type 'feedback' to rate this response)\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Try to find the dataset path\n",
        "    dataset_paths = [\n",
        "        \"/root/.cache/kagglehub/datasets/crispen5gar/recipes3k/versions/1/recipes.json\",\n",
        "        \"./recipes.json\"\n",
        "    ]\n",
        "\n",
        "    kb_path = None\n",
        "    for path in dataset_paths:\n",
        "        if os.path.exists(path):\n",
        "            kb_path = path\n",
        "            break\n",
        "\n",
        "    if not kb_path:\n",
        "        print(\"Warning: Could not find dataset file. Will attempt to download.\")\n",
        "\n",
        "    # Initialize chatbot with CPU-optimized settings\n",
        "    chatbot = HybridChatbot(\n",
        "        paper_url=\"https://yourknow.com/uploads/books/5dd0f9604c895.pdf\",\n",
        "        dataset_path=kb_path,\n",
        "        model_name=\"gpt2-medium\",  # Consider using a smaller model for CPU like \"distilgpt2\"\n",
        "        enable_rlhf=True,\n",
        "        rlhf_update_threshold=10  # Update model after 10 samples\n",
        "    )\n",
        "\n",
        "    # Test with sample questions\n",
        "    test_questions = [\n",
        "        \"How to make milk tart?\",\n",
        "        \"Recipe for Rusks\",\n",
        "        \"Hi, I need help with Egg Avocado Open Sandwich\",\n",
        "        \"Ingredients and recipe for Spicy Chicken Avocado wrap\",\n",
        "        \"I need the South african Biltong Pasta potjie recipe?\"\n",
        "    ]\n",
        "\n",
        "    for question in test_questions:\n",
        "        print(f\"Q: {question}\")\n",
        "        result = chatbot.get_answer111(question)\n",
        "        print(f\"A: {result['answer']}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # Run interactive chat mode\n",
        "    #chatbot.chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9wuxbsBiNBn",
        "outputId": "4a5309e8-21c5-4765-d7b2-e5d7f42e5873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing CPU-optimized Chatbot...\n",
            "Loading dataset from: /root/.cache/kagglehub/datasets/crispen5gar/recipes3k/versions/1/recipes.json\n",
            "Loading PDF from: https://yourknow.com/uploads/books/5dd0f9604c895.pdf\n",
            "Loading existing vector database from: hybrid_vectors\n",
            "Loading language model: gpt2-medium\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up language model pipeline...\n",
            "Chatbot initialization complete!\n",
            "Q: How to make milk tart?\n",
            "Feedback saved to rlhf_feedback.json\n",
            "A: \n",
            "        I am a Chatbot assistant that can answer questions about South African Food recipes and some general recipes!\n",
            "\n",
            "        \n",
            "\n",
            "        QUESTION:\n",
            "        How to make milk tart?\n",
            "\n",
            "        ANSWER:\n",
            "         What is the difference between sugar, honey or cream cheese in this recipe?\n",
            "\n",
            "  Question :    \"How do you know if your buttercream will be too soft?\" Answer :         You need to use 1/2 cup of melted fat. The more saturated fats are used, it's easier for them not melt into each other when they're mixed with water (like coconut oil). If there isn't enough melting point on top, then its hard to get all the oils from one piece together so just add as much extra cooking liquid like 2 tablespoons at first before adding any additional ingredients until everything melts evenly. It should look something like below after mixing up my mixture...\n",
            "                          This was really easy because we had already made our own whipped cream which worked out great but still needed adjusting later - see how different things looked once blended? So here goes... First off let me say thank goodness i didn´t have to buy anything else since these were homemade! They came pre-made ready milled by hand using only 3 cups of flour, no baking powder etc.. And yes those biscuits did come very well packed inside :) But what makes their taste even better than others?? Well thanks again guys!! We'll definitely try making another batch soon!! Thank u!!! Reply Delete Comment Add comment 0 comments Read Comments » Leave a review | View previous reviews <<< Previous Review By Anonymous On Nov 17th 2017 @ 12 PM CST Rating:-5 Thanks everyone who participated today....I'm glad someone took time away from work tonight.....and got back early tomorrow morning......it seems people enjoyed themselves alot yesterday....thank yall!!!!!!!!! Posted by james_bobby on Sep 26nd 2016 @ 10 AM EST Rating:: 5 Love love LOVE IT!!!!!!!! SO GOOD TO SEE YOU ALL COMING BACK FOR MORE SHOWS AND FUNS WITH US...............THANKYOU THANK YOU EVERYONE WHO JOINED TODAY AT 9AM THIS MORNING..WE WILL BE WORKING ON A NEW SHOW IN THE NEXT MONNDAY......PLEASE CHECK OUR PAGE AGAINST EMAIL OR CALL IF WE ARE NOT UP BY THAT TIME.................IF THERE IS ANYTHING ELSE NEEDED PLEASE LET ME KNOW ASap................….thanks Again James B*******Posted Date Jul 27st 2015@ 11PM PSTRating 4 stars(outoffive)Thanks Everyone Who Participated Today In Our New Show Tonight At 8pm ET Join us For More Shows & Fun With Us........................Thankyou To All Of Those That Came Out Tomorrow Evening................For Your Support AswellAsToThoseThat Didn`t Come Back After Work Day Because Of Weather Or Other Reasons\n",
            "--------------------------------------------------\n",
            "Q: Recipe for Rusks\n",
            "Error processing query: index out of range in self\n",
            "A: I'm sorry, I encountered an error while processing your question.\n",
            "--------------------------------------------------\n",
            "Q: Hi, I need help with Egg Avocado Open Sandwich\n",
            "Feedback saved to rlhf_feedback.json\n",
            "A: \n",
            "        I am a Chatbot assistant that can answer questions about South African Food recipes and some general recipes!\n",
            "\n",
            "        \n",
            "\n",
            "        QUESTION:\n",
            "        Hi, I need help with Egg Avocado Open Sandwich\n",
            "\n",
            "        ANSWER:\n",
            "         The recipe is simple. You just add 1 egg to the avocado mixture then mix it together until you get an emulsion of eggs all over your sandwich. It's great on toast or sandwiches but also good as side dish for dinner in restaurants where they don't serve any meat so use this instead if possible. This will give them something different from what we usually have at home! If you are looking into making avocados open sesame breads like these, please feel free :) Thanks again!! :) Reply Delete Email Please enter email address below To unsubscribe click here Cancel Subscribe Your information has been sent successfully Check my junk mail box... Thank YOU!!! We'll send another message when new messages arrive.. Send me more info soon? Yes No Comment By clicking Register above (or by using our form),you agree not onlyto receive emails containing news content suchas updates & special offers;but alsothat there may be no spambait articles appearing within e-mail newslettersfrom timeTotimeand other similar communications which donot contain unsolicited commercial advertising - opt-outat any pointby following the instructions providedon each page(if applicable).By submittingthis notification through social media sites/apps (\"Sites\") including Facebook, Twitter,... [More] View full article » Leave A comment Here | Read More Comments Welcome back everyone who visited us recently....I hope everything was ok out there.....You know how many times i tell people \"don´t worry\" because things seem fine? Well thats exactly why today its very important...because tomorrow could change alot....So lets start off right now......First thing first let`ve talk about food..so before going further......Let�re clear up one big misconception around SA...............It seems most Southerners dont really understand their own culture!! They think anything imported must be bad since nothing tastes better than eating local foods made locally.........Well actually NO!!!! There are plentyof ways to make delicious dishes without having to import stuff even though those same ingredients would normally cost less money elsewhere….like buying fresh produce etc…which means lots cheaper prices everywhere else too…..And yes Im sure everyones favourite breakfast items were always cooked well done anyway……But still cant believe anyone thinks importing shit makes sense unless someone told him otherwise …Just kidding guys ;-) So next stop being ignorant idiots,,let´r go ahead say hello once again................Now im gonna sharewith yall two amazing meals prepared entirely domestically…………This morning after work lunch came along way sooner rather later compared wth yesterday evening meal........Its called 'Egg Salad' And while she didnt ask her husband whether he wanted his wife to eat hers tonight,,,,he did want to try somethingsnewy tasty nonetheless lol........................................................If u havent tried EGG SALAD yet checkit online www http://www1.\"http://eagelash.com/?p=5&qid=/search_query\". Then follow link under menu item name eg ''egg salad'' Or searchfor ingredient names ie �avocado chicken soup''. Now take 2 tbsp water + ½ tsp salt plus ¼ cup chopped onion finely chop garlic cloves till fragrant Add half red pepper flakes lightly fry onions til golden brown Pour oil slowly onto pan Heat gently stirring constantly adding additional tablespoonfulwater gradually pouring hot liquid evenly Over medium heat cook vegetables thoroughly For 5 minutes turn down flame Remove lid carefully stir occasionally Cook remaining 4 mins addedgarlic powder Once veggiesare tender remove foil cover top generously place lettuce leaves aside In large bowl combine spinach,chicken broth spices blendwelladd extra coconut milk pinch cayenne crackly spice Mixing quickly pour batter Into greased baking tray Bake uncovered 30minsFor serving sprinkle sliced almonds On plate garnishWith freshly grated nutmeg\n",
            "--------------------------------------------------\n",
            "Q: Ingredients and recipe for Spicy Chicken Avocado wrap\n",
            "Feedback saved to rlhf_feedback.json\n",
            "A: \n",
            "        I am a Chatbot assistant that can answer questions about South African Food recipes and some general recipes!\n",
            "\n",
            "        \n",
            "\n",
            "        QUESTION:\n",
            "        Ingredients and recipe for Spicy Chicken Avocado wrap\n",
            "\n",
            "        ANSWER:\n",
            "         Recipe is very simple, just mix together the ingredients in your food processor. You will get thick sauce with lots of avocado slices on top! It's great to serve as appetizer or side dish at dinner time too!! The best thing you can do after eating it? Just add more chicken strips, if desired. Enjoy!! Reply Delete Quote Save To My Recipes Share On Pinterest Write A Comment And share this post using social media buttons below : ) Thankyou so much!!! :) Have fun making these amazing dishes!!!! xo - Mimi & Rachael (M) @mimikitchen #spicyfoods http://www-msnbc3d.com/blogs/?p=128079&sid=1#post_15404724 https:/ / www2...rchaelb@gmail....tteaafoodrecipes...and here are my other posts from last year ;-) Thanks again everyone who visit me online ;) Hope yall enjoy them all :-) Happy Cooking xxx Posted by mimsimonstercooking | September 22nd 2017 11:54 AM Read More Comments » Submit Your Own Blog Post Here 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Next Page >> View All Posts From This Author Message Bookmark +\n",
            " Share With Others Facebook Twitter Google+ Email Print Delicious Digg Reddit StumbleUpon Tumblr Mail Yummly Telegram Pocket Flipboard LinkedIn Pin Again Copy Link Copied Embed Code Powered By GrabEase Inc. Free Personalize Website Profile Handyman Tools Copyright © 2018 Grocery Hookup LLC ALL rights reserved. Do not reproduce without permission. No part may be reproduced except under limited circumstances including but NOT LIMITED TO COPYING FOR NONcommercial purposes only If you have any comments please contact us via email [email protected] We respect your privacy therefor Please enable JavaScript to view our comment policy before posting Click \"I agree\" when finished reading content Check out what others think About Me Subscribe Now For free we send over $100 worth Of groceries every month Get instant access to 50% off everything delivered directly to YOUR door Sign up now > See how many people subscribe today < Prev Top Bottom Left Right Back Continue Contents Home Menu Search Results Categories Dishes Prepared Foods Other Items Specialty Restaurants Kitchen Accessories Baking Supplies Snacks Soups Salads Pastries Meat Products Cookbooks Vegetables Seafeed Produce Wine Shops Gifts Shopping Coupons Deals Events HowTo Guides Books News Articles Reviews Apparel Clothing Shoes Bathroom Decorations Hair Care Miscellaneous Craft Projects DIY Services Artwork Design Photography Music Video Games Kids Toys Sports Gear Baby Stuff Flowers Plants Pets Pet Treatments Pottery Furniture Appliances Watches Travel Jewelry Gift Cards\n",
            "--------------------------------------------------\n",
            "Q: I need the South african Biltong Pasta potjie recipe?\n",
            "Feedback saved to rlhf_feedback.json\n",
            "A: \n",
            "        I am a Chatbot assistant that can answer questions about South African Food recipes and some general recipes!\n",
            "\n",
            "        \n",
            "\n",
            "        QUESTION:\n",
            "        I need the South african Biltong Pasta potjie recipe?\n",
            "\n",
            "        ANSWER:\n",
            "          THE RECIPE IS HERE, BUT YOU NEED TO READ IT FIRST. IF YOU DO NOT HAVE A SINGLE POTJIE IN YOUR HOUSE AND ARE STILL TIRED OF THIS THREAD PLEASE COMMENT ON WHAT TYPE OR MODEL MEANS FOR US to make this dish for you (or if it's just something we want) or let us know what kind of pasta would be best in your kitchen so we could try out different ideas on how many ingredients are needed depending upon which type of sauce is used...we're always open-minded when making new dishes :) THANKS SO MUCH!! <3 Reply Delete Post\n",
            "Hi there - i'm looking forward reading all these comments because they really help me understand my cooking process better! thanks again :D Thank You very much!!! :-) Hi there –I have been trying various sauces over time but never found one with enough flavor & texture without using too little salt. So today, i decided …to experiment by adding 1/2 tsp extra sea salt into each batch as well as 2 tbsp coconut oil instead..and voila...it was perfect!! It made everything taste even more delicious than before ;-) Thanks also @mikebondi who helped me find exactly where he uses ¼ cup + ½ teaspoon plus 3 drops per serving = 6 teaspoons total amount (= 4 tablespoons). And thank goodness —that makes them easy… ;) Cheers xo ♥ Mika ☆️ http://www.\"pinterest\" Pinterest \"http:/ / www.\"facebook\".com/\"MikaelleBONDISTAVEONLINE\" Twitter \"@kimba_tasteoffoods\" Instagram \"http:\\ // instagram@glamourmagazine\" Blogspot https:\"blogspotapp.net/ michaellindsayyamu\">https=\"blogspoon.com/?utm_source=twitter&utm%3Dwordpress#!/ kimbaltspoon </td> <!--<!--[if gte IE 9]><script src=\"//ajax.googleapis;noredirect url='//blogsites.msdncdn..js'></Script><div class=\\\"ft-adidas__branding{margin:-50px 0 20px 40%;padding:#000;}\"><img alt=\"\" width=\"/images\\/ads\\footer\\/\\/wp1″ border=\"0\"></iframe></div--></center>\" [url=javascript:void(0);} ] [/center][fontc=#9C7FFF]||||||| | ||[/size][/font][/center]\"\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "laI0C4S6AW3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------Define the chatbot App-----------------------------\n",
        "\n",
        "def chatbot_response(user_input):\n",
        "    result = chatbot.get_answer(user_input)\n",
        "    return result[\"answer\"]\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=chatbot_response,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask a question about South African Food receipes...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Traditional South African Food Receipes\",\n",
        "    description=\"Ask questions about South African Food Receipes.\"\n",
        ")\n",
        "\n",
        "interface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "-P61Yfbsq4Av",
        "outputId": "5454ac39-948f-4818-f611-8566042e1e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://653a27c0af95b5c461.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://653a27c0af95b5c461.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    }
  ]
}