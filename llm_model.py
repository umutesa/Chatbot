# -*- coding: utf-8 -*-
"""llm_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kokgPge5RTqd347m5ozi1kt5xMHR3qQj
"""

pip install langchain_community

pip install faiss-cpu

!pip install gradio --quiet

pip install sentence-transformers

pip install langchain_openai

pip install pypdf

pip install langchain

#--------------------LIABARIES-------------------------------------------

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

import os
import json
import kagglehub
import pandas as pd
from typing import List, Dict, Any, Optional, Union

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.llms import HuggingFacePipeline
from langchain.docstore.document import Document
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline


import numpy as np
import random
from datetime import datetime

from transformers import Trainer, TrainingArguments
from typing import Tuple, List, Dict
from langchain.retrievers import MultiQueryRetriever
from langchain.retrievers import EnsembleRetriever

import gradio as gr

import os
import json
import random
import torch
import kagglehub
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any

#------------------------------------------------------Download Dataset--------------------------------------------

path = kagglehub.dataset_download("crispen5gar/recipes3k")
json_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith(".json")]
for file in json_files:
    print(file)

#-----------------------------------------------------------------Defining Model-------------------------------------------------

os.environ["CUDA_VISIBLE_DEVICES"] = ""

"""Class to store and manage human feedback for reinforcement learning."""
class RLHFFeedback:
    def __init__(self, feedback_file: str = "rlhf_feedback.json"):
        self.feedback_file = feedback_file
        self.feedback_data = self._load_feedback()

    def _load_feedback(self) -> Dict:
        """Load existing feedback data or create a new structure."""
        if os.path.exists(self.feedback_file):
            try:
                with open(self.feedback_file, 'r') as f:
                    return json.load(f)
            except json.JSONDecodeError:
                print(f"Error loading feedback file. Creating new feedback database.")

        # Initial structure
        return {
            "interactions": [],
            "preferred_responses": [],
            "rejected_responses": [],
            "model_updates": []
        }

    def save_feedback(self):
        """Save feedback data to file."""
        with open(self.feedback_file, 'w') as f:
            json.dump(self.feedback_data, f, indent=2)
        print(f"Feedback saved to {self.feedback_file}")

    def add_interaction(self, query: str, response: str, source_types: List[str]):
        """Add a new interaction without feedback yet."""
        self.feedback_data["interactions"].append({
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "response": response,
            "source_types": source_types,
            "feedback_requested": True,
            "feedback_provided": False
        })
        self.save_feedback()
        return len(self.feedback_data["interactions"]) - 1  # Return index for reference

    def add_feedback(self, interaction_idx: int, rating: int, alternative_response: Optional[str] = None):
        """Add human feedback to an existing interaction."""
        if 0 <= interaction_idx < len(self.feedback_data["interactions"]):
            interaction = self.feedback_data["interactions"][interaction_idx]
            interaction["feedback_provided"] = True
            interaction["rating"] = rating
            interaction["feedback_timestamp"] = datetime.now().isoformat()

            if rating >= 4:  # Rating of 4 or 5 (on a 1-5 scale)
                self.feedback_data["preferred_responses"].append({
                    "query": interaction["query"],
                    "response": interaction["response"],
                    "rating": rating,
                    "source_types": interaction["source_types"]
                })
            elif rating <= 2:  # Rating of 1 or 2
                self.feedback_data["rejected_responses"].append({
                    "query": interaction["query"],
                    "response": interaction["response"],
                    "rating": rating,
                    "source_types": interaction["source_types"],
                    "alternative_response": alternative_response
                })

            self.save_feedback()
            return True
        return False

    def get_training_pairs(self, min_samples: int = 5) -> Tuple[List[Dict], List[Dict], bool]:
        """Get training data pairs for RLHF fine-tuning."""
        preferred = self.feedback_data["preferred_responses"]
        rejected = self.feedback_data["rejected_responses"]

        # Only return data if we have enough samples
        if len(preferred) >= min_samples and len(rejected) >= min_samples:
            return preferred, rejected, True

        return [], [], False

    def record_model_update(self, update_details: Dict):
        """Record when the model was updated with RLHF."""
        update_record = {
            "timestamp": datetime.now().isoformat(),
            "details": update_details,
            "samples_used": {
                "preferred": len(self.feedback_data["preferred_responses"]),
                "rejected": len(self.feedback_data["rejected_responses"])
            }
        }
        self.feedback_data["model_updates"].append(update_record)
        self.save_feedback()


class HybridChatbot:
    def __init__(self,
                 paper_url: str = "https://yourknow.com/uploads/books/5dd0f9604c895.pdf",
                 dataset_path: str = None,
                 model_name: str = "gpt2-medium",
                 embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2",
                 vector_db_path: str = "hybrid_vectors",
                 enable_rlhf: bool = True,
                 feedback_file: str = "rlhf_feedback.json",
                 rlhf_update_threshold: int = 20):

        self.model_name = model_name
        self.vector_db_path = vector_db_path
        self.paper_url = paper_url
        self.enable_rlhf = enable_rlhf
        self.rlhf_update_threshold = rlhf_update_threshold

        print("Initializing CPU-optimized Chatbot...")

        # Initialize RLHF feedback system
        if self.enable_rlhf:
            self.feedback_system = RLHFFeedback(feedback_file)

        # 1. Load and prepare dataset
        if dataset_path:
            self.dataset_path = dataset_path
        else:
            # Download the dataset from Kaggle
            self.dataset_path = self._download_dataset()

        self._intents = self._load_dataset()

        # 2. Load PDF and prepare for document retrieval
        self.pdf_docs = self._load_and_split_pdf()

        # 3. Initialize embedding model - with CPU optimizations
        self.embedding_model = HuggingFaceEmbeddings(
            model_name=embedding_model,
            model_kwargs={"device": "cpu"},
            encode_kwargs={"batch_size": 8}  # Smaller batch size for CPU
        )

        # 4. Create/load vector database for retrieval
        self.vector_db = self._setup_vector_db()

        # 5. Initialize language model with CPU optimizations
        print(f"Loading language model: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        # Force CPU and use half precision to save memory
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="cpu",
            torch_dtype=torch.float32  # Using float32 for CPU compatibility
        )

        # 6. Set up the pipeline with optimized settings for CPU
        self.llm_pipeline = self._setup_llm_pipeline()

        # 7. Create QA chain
        self.qa_chain = self._setup_qa_chain()

        print("Chatbot initialization complete!")

    def _download_dataset(self):
        """Download the dataset from Kaggle."""
        try:
            path = kagglehub.dataset_download("crispen5gar/recipes3k")
            data_file = os.path.join(path, "recipes.json")
            return data_file
        except Exception as e:
            print(f"Error downloading dataset: {e}")
            return None

    def _load_dataset(self):
        """Load the dataset knowledge base."""
        try:
            print(f"Loading dataset from: {self.dataset_path}")
            with open(self.dataset_path, 'r') as file:
                intents = json.load(file)
            return intents
        except Exception as e:
            print(f"Error loading Dataset knowledge base: {e}")
            # Create empty intents if loading fails
            return {"intents": []}

    def _load_and_split_pdf(self):
        """Load and split the research paper PDF."""
        try:
            print(f"Loading PDF from: {self.paper_url}")
            loader = PyPDFLoader(self.paper_url)
            pages = loader.load()
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=2000,
                chunk_overlap=700
            )
            return text_splitter.split_documents(pages)
        except Exception as e:
            print(f"Error loading PDF: {e}")
            return []  # Return empty list if loading fails

    def _convert_dataset_to_documents(self):
        """Convert intents to Document objects for vector storage."""
        documents = []
        for intent in self._intents:  # Remove the .get() since self._intents is already a list
          for pattern in intent.get("patterns", []):
            documents.append(Document(page_content=pattern, metadata={"source": "dataset_kb", **intent}))
        return documents

    def _setup_vector_db(self):
        """Set up the vector database for hybrid retrieval."""
        # Check if vector database already exists
        if os.path.exists(self.vector_db_path):
            print(f"Loading existing vector database from: {self.vector_db_path}")
            return FAISS.load_local(self.vector_db_path, self.embedding_model, allow_dangerous_deserialization=True)

        print("Creating new vector database...")
        # Combine PDF documents and dataset documents
        dataset_docs = self._convert_dataset_to_documents()
        all_docs = self.pdf_docs + dataset_docs

        # Process in smaller batches to optimize for CPU
        batch_size = 32
        all_batches = [all_docs[i:i + batch_size] for i in range(0, len(all_docs), batch_size)]

        # Create vector database with the first batch
        if len(all_batches) > 0 and len(all_batches[0]) > 0:
            vector_db = FAISS.from_documents(all_batches[0], self.embedding_model)

            # Add remaining batches
            for i, batch in enumerate(all_batches[1:], 1):
                if batch:  # Check if batch is not empty
                    print(f"Processing batch {i+1}/{len(all_batches)}...")
                    vector_db.add_documents(batch)
        else:
            # Fallback if no documents were processed
            print("Warning: No documents to process for vector database")
            vector_db = FAISS.from_texts(["Empty database placeholder"], self.embedding_model)

        # Save for future use
        print(f"Saving vector database to: {self.vector_db_path}")
        vector_db.save_local(self.vector_db_path)

        return vector_db

    def _setup_llm_pipeline(self):
        """Set up the language model pipeline optimized for CPU."""
        print("Setting up language model pipeline...")
        # CPU-optimized parameters: Lower max_length, increase pad_token_id consistency
        llm_pipeline = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            max_length=6000,  # Reduced for CPU efficiency
            min_length=600,
            temperature=0.30,
            top_p=0.95,
            top_k=50,
            repetition_penalty=1.2,
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id, # Force CPU
            truncation=True
        )

        return HuggingFacePipeline(pipeline=llm_pipeline)

    def _setup_qa_chain(self):
        """Set up the QA chain with prompt template."""
        # Create prompt template
        prompt_template = """
        I am a Chatbot assistant that can answer questions about South African Food recipes and some general recipes!

        {context}

        QUESTION:
        {question}

        ANSWER:
        """

        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )

        return RetrievalQA.from_chain_type(
            llm=self.llm_pipeline,
            chain_type="stuff",
            retriever=self.vector_db.as_retriever(search_kwargs={"k": 3}),
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt}
        )


    def format_answer(self, answer: str) -> str:
      """Clean and format the answer."""
      unwanted_phrases = [
        "I am a Chatbot assistant that can",
        "QUESTION:",
        "ANSWER:",
      ]
      for phrase in unwanted_phrases:
        answer = answer.replace(phrase, "")

    # Trim leading/trailing whitespace
      answer = answer

    # Ensure a complete sentence
      if not answer.endswith("."):
        answer += "."

      return answer

    def get_answer111(self, query: str) -> Dict[str, Any]:
      """Get answer for the given query."""
      try:
            # Set up retrievers for both dataset and paper
        dataset_retriever = self.vector_db.as_retriever(search_kwargs={"k": 4, "filter": {"source": "dataset_kb"}})
        paper_retriever = self.vector_db.as_retriever(search_kwargs={"k": 4, "filter": {"source": "paper"}})

        self.qa_chain.retriever = paper_retriever
        if not self.qa_chain.retriever:
          self.qa_chain.retriever = dataset_retriever

        # Generate answer
        result = self.qa_chain({"query": query})
        answer = result.get('result', '')

        # Format the answer
        answer = answer

        # Identify source type
        source_types = []
        for doc in result.get('source_documents', []):
            source = doc.metadata.get('source', '')
            if source == "dataset_kb":
                source_types.append("dataset")
            else:
                source_types.append("paper")

        # Deduplicate source types
        source_types = list(set(source_types))

        # Store interaction for feedback
        interaction_idx = -1
        if self.enable_rlhf:
            interaction_idx = self.feedback_system.add_interaction(query, answer, source_types)

        return {
            "answer": answer,
            "source_types": source_types,
            "interaction_idx": interaction_idx
        }
      except Exception as e:
        print(f"Error processing query: {e}")
        return {
            "answer": "I'm sorry, I encountered an error while processing your question.",
            "source_types": [],
            "interaction_idx": -1
        }

    def provide_feedback(self, interaction_idx: int, rating: int, alternative_response: Optional[str] = None) -> bool:
        """Add human feedback for a specific interaction."""
        if not self.enable_rlhf:
            return False

        success = self.feedback_system.add_feedback(interaction_idx, rating, alternative_response)

        # Check if we should update the model based on feedback
        if success:
            preferred, rejected, should_update = self.feedback_system.get_training_pairs(
                min_samples=self.rlhf_update_threshold
            )

            if should_update:
                print(f"Sufficient feedback collected ({len(preferred)} preferred, {len(rejected)} rejected). Updating model...")
                self._update_model_with_rlhf(preferred, rejected)
            else:
                remaining = self.rlhf_update_threshold - min(len(preferred), len(rejected))
                print(f"Need {remaining} more feedback samples before model update")

        return success

    def _prepare_rlhf_training_data(self, preferred: List[Dict], rejected: List[Dict]):
        """Prepare data for RLHF training."""
        # Create paired examples for preference learning
        training_pairs = []

        # This simplified approach pairs random preferred and rejected responses
        # Limit the number of pairs for CPU efficiency
        max_pairs = min(len(preferred), len(rejected), 50)  # Cap at 50 pairs for CPU efficiency

        for _ in range(max_pairs):
            pref = random.choice(preferred)
            rej = random.choice(rejected)

            training_pairs.append({
                "query": pref["query"],
                "chosen": pref["response"],
                "rejected": rej["response"]
            })

        return training_pairs

    def _update_model_with_rlhf(self, preferred: List[Dict], rejected: List[Dict]):
        """Update the language model using RLHF techniques - optimized for CPU."""
        try:
            print("Starting RLHF model update with CPU optimizations...")

            # 1. Prepare data
            training_pairs = self._prepare_rlhf_training_data(preferred, rejected)

            # 2. Create a custom dataset for preference learning
            class PreferenceDataset(torch.utils.data.Dataset):
                def __init__(self, pairs, tokenizer, max_length=256):  # Reduced max_length for CPU
                    self.pairs = pairs
                    self.tokenizer = tokenizer
                    self.max_length = max_length

                def __len__(self):
                    return len(self.pairs)

                def __getitem__(self, idx):
                    pair = self.pairs[idx]

                    # Format inputs
                    chosen_text = f"Query: {pair['query']}\nResponse: {pair['chosen']}"
                    rejected_text = f"Query: {pair['query']}\nResponse: {pair['rejected']}"

                    # Tokenize with reduced padding for CPU efficiency
                    chosen_tokens = self.tokenizer(
                        chosen_text,
                        truncation=True,
                        max_length=self.max_length,
                        padding="max_length",
                        return_tensors="pt"
                    )

                    rejected_tokens = self.tokenizer(
                        rejected_text,
                        truncation=True,
                        max_length=self.max_length,
                        padding="max_length",
                        return_tensors="pt"
                    )

                    # Create preference labels - 1 for chosen, 0 for rejected
                    return {
                        "chosen_input_ids": chosen_tokens.input_ids.squeeze(),
                        "chosen_attention_mask": chosen_tokens.attention_mask.squeeze(),
                        "rejected_input_ids": rejected_tokens.input_ids.squeeze(),
                        "rejected_attention_mask": rejected_tokens.attention_mask.squeeze(),
                    }

            # 3. Create dataset
            dataset = PreferenceDataset(training_pairs, self.tokenizer)
            print(f"Created preference dataset with {len(dataset)} examples")

            # Create temporary model directory
            output_dir = "rlhf_updated_model"
            os.makedirs(output_dir, exist_ok=True)

            # 4. Training arguments optimized for CPU
            training_args = TrainingArguments(
                output_dir=output_dir,
                num_train_epochs=2,  # Reduced for CPU
                per_device_train_batch_size=1,  # Smaller batch size for CPU
                gradient_accumulation_steps=4,  # Accumulate gradients to compensate for small batch size
                save_steps=50,
                save_total_limit=1,  # Save disk space
                logging_steps=10,
                learning_rate=2e-5,  # Lower learning rate for stability
                weight_decay=0.01,  # Regularization
                no_cuda=True,  # Force CPU
                dataloader_num_workers=1,  # Limit workers for CPU
                evaluation_strategy="no",  # No eval to save time
                fp16=False,  # No mixed precision on CPU
            )

            # 5. Define training logic
            class PreferenceTrainer(Trainer):
                def compute_loss(self, model, inputs, return_outputs=False):
                    # Process in smaller chunks to avoid OOM on CPU
                    # Get logits for chosen and rejected responses
                    chosen_outputs = model(
                        input_ids=inputs["chosen_input_ids"],
                        attention_mask=inputs["chosen_attention_mask"]
                    )
                    rejected_outputs = model(
                        input_ids=inputs["rejected_input_ids"],
                        attention_mask=inputs["rejected_attention_mask"]
                    )

                    # Extract logits
                    chosen_logits = chosen_outputs.logits
                    rejected_logits = rejected_outputs.logits

                    # Simple preference loss: make chosen more likely than rejected
                    # This is a simplified version of RLHF loss
                    chosen_log_probs = self._get_sequence_log_probs(
                        chosen_logits, inputs["chosen_input_ids"], inputs["chosen_attention_mask"]
                    )
                    rejected_log_probs = self._get_sequence_log_probs(
                        rejected_logits, inputs["rejected_input_ids"], inputs["rejected_attention_mask"]
                    )

                    # Preference loss: maximize difference between chosen and rejected
                    # Using stable softplus for numerical stability on CPU
                    logits_diff = chosen_log_probs - rejected_log_probs
                    loss = -torch.mean(torch.log(torch.sigmoid(logits_diff) + 1e-5))

                    if return_outputs:
                        return loss, (chosen_outputs, rejected_outputs)
                    return loss

                def _get_sequence_log_probs(self, logits, input_ids, attention_mask):
                    # Calculate log probs for the sequence with memory optimizations
                    shift_logits = logits[..., :-1, :].contiguous()
                    shift_labels = input_ids[..., 1:].contiguous()

                    # Get log probs using log_softmax with memory-efficient chunks
                    log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)

                    # Gather the log probs corresponding to the labels
                    gathered_log_probs = log_probs.gather(-1, shift_labels.unsqueeze(-1)).squeeze(-1)

                    # Mask out padding
                    shift_mask = attention_mask[..., 1:].contiguous()
                    masked_log_probs = gathered_log_probs * shift_mask

                    # Sum log probs and normalize by sequence length
                    seq_lengths = torch.sum(shift_mask, dim=-1)
                    seq_log_probs = torch.sum(masked_log_probs, dim=-1) / (seq_lengths + 1e-6)

                    return seq_log_probs

            # 6. Initialize trainer
            trainer = PreferenceTrainer(
                model=self.model,
                args=training_args,
                train_dataset=dataset,
                tokenizer=self.tokenizer
            )

            # 7. Train the model with progress reporting
            print("Starting training...")
            trainer.train()
            print("Training complete!")

            # 8. Save and update the model
            trainer.save_model(output_dir)
            print(f"RLHF-updated model saved to {output_dir}")

            # 9. Update the pipeline with new model - reload with CPU optimizations
            self.model = AutoModelForCausalLM.from_pretrained(
                output_dir,
                device_map="cpu",
                torch_dtype=torch.float32
            )
            self.llm_pipeline = self._setup_llm_pipeline()
            self.qa_chain = self._setup_qa_chain()

            # 10. Record the update
            self.feedback_system.record_model_update({
                "update_time": datetime.now().isoformat(),
                "training_pairs": len(training_pairs),
                "training_epochs": training_args.num_train_epochs
            })

            print("Model successfully updated with RLHF!")
            return True

        except Exception as e:
            print(f"Error during RLHF update: {e}")
            import traceback
            traceback.print_exc()
            return False

    def chat(self):
        """Interactive chat mode with RLHF feedback collection."""
        print("This chatbot can answer questions about South African Food recipes")
        print("Type 'exit' to quit")
        if self.enable_rlhf:
            print("Type 'feedback' after a response to provide feedback")
        print("===================================\n")

        while True:
            user_input = input("You: ")
            if user_input.lower() in ['exit', 'quit', 'bye']:
                print("Chatbot: Goodbye!")
                break

            if user_input.lower() == 'feedback' and self.enable_rlhf and hasattr(self, 'last_interaction_idx'):
                # Get feedback
                try:
                    rating = int(input("Please rate the last response (1-5, 5 being best): "))
                    if 1 <= rating <= 5:
                        alt_response = None
                        if rating <= 2:  # For negative ratings, ask for alternative response
                            alt_response = input("Please suggest a better response (optional): ")
                            if not alt_response.strip():
                                alt_response = None

                        success = self.provide_feedback(self.last_interaction_idx, rating, alt_response)
                        if success:
                            print("Thank you for your feedback!")
                        else:
                            print("Failed to record feedback.")
                    else:
                        print("Invalid rating. Please use a scale of 1-5.")
                except ValueError:
                    print("Invalid input. Please enter a number between 1 and 5.")
                continue

            response = self.get_answer(user_input)
            print(f"\nChatbot: {response['answer']}\n")

            # Store interaction index for feedback
            if self.enable_rlhf:
                self.last_interaction_idx = response['interaction_idx']
                print("(Type 'feedback' to rate this response)")


if __name__ == "__main__":
    # Try to find the dataset path
    dataset_paths = [
        "/root/.cache/kagglehub/datasets/crispen5gar/recipes3k/versions/1/recipes.json",
        "./recipes.json"
    ]

    kb_path = None
    for path in dataset_paths:
        if os.path.exists(path):
            kb_path = path
            break

    if not kb_path:
        print("Warning: Could not find dataset file. Will attempt to download.")

    # Initialize chatbot with CPU-optimized settings
    chatbot = HybridChatbot(
        paper_url="https://yourknow.com/uploads/books/5dd0f9604c895.pdf",
        dataset_path=kb_path,
        model_name="gpt2-medium",  # Consider using a smaller model for CPU like "distilgpt2"
        enable_rlhf=True,
        rlhf_update_threshold=10  # Update model after 10 samples
    )

    # Test with sample questions
    test_questions = [
        "How to make milk tart?",
        "Recipe for Rusks",
        "Hi, I need help with Egg Avocado Open Sandwich",
        "Ingredients and recipe for Spicy Chicken Avocado wrap",
        "I need the South african Biltong Pasta potjie recipe?"
    ]

    for question in test_questions:
        print(f"Q: {question}")
        result = chatbot.get_answer111(question)
        print(f"A: {result['answer']}")
        print("-" * 50)

    # Run interactive chat mode
    #chatbot.chat()

# ------------------------------Define the chatbot App-----------------------------

def chatbot_response(user_input):
    result = chatbot.get_answer(user_input)
    return result["answer"]

interface = gr.Interface(
    fn=chatbot_response,
    inputs=gr.Textbox(lines=2, placeholder="Ask a question about South African Food receipes..."),
    outputs="text",
    title="Traditional South African Food Receipes",
    description="Ask questions about South African Food Receipes."
)

interface.launch(share=True)